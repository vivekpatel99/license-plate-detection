{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License Plate Recognition\n",
    "\n",
    "@misc{\n",
    "license-plate-recognition-rxg4e_dataset,\n",
    "title = { License Plate Recognition Dataset },\n",
    "type = { Open Source Dataset },\n",
    "author = { Roboflow Universe Projects },\n",
    "howpublished = { \\url{ https://universe.roboflow.com/roboflow-universe-projects/license-plate-recognition-rxg4e } },\n",
    "url = { https://universe.roboflow.com/roboflow-universe-projects/license-plate-recognition-rxg4e },\n",
    "journal = { Roboflow Universe },\n",
    "publisher = { Roboflow },\n",
    "year = { 2024 },\n",
    "month = { oct },\n",
    "note = { visited on 2025-02-17 },\n",
    "}\n",
    "\n",
    "* References\n",
    "    1. https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb\n",
    "    2. https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#evaluating-the-model-optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')], '2.15.1')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "found_gpu = tf.config.list_physical_devices('GPU')\n",
    "if not found_gpu:\n",
    "    raise Exception(\"No GPU found\")\n",
    "found_gpu, tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the label map utility module\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "# import module for reading and updating configuration files.\n",
    "from object_detection.utils import config_util\n",
    "\n",
    "# import module for visualization. use the alias `viz_utils`\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "# import module for building the detection model\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload dotenv \n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# auto reload libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization_funcs import plot_random_images_bbox\n",
    "from utils.annotation_processor import AnnotationProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/License-Plate-Recognition-8/train/\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "\n",
    "# https://gist.github.com/bdsaglam/586704a98336a0cf0a65a6e7c247d248\n",
    "\n",
    "with initialize(version_base=None, config_path=\"conf\"):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    print(cfg.DATASET_DIRS.TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OUPUT_DIR': 'exported_models/my_${PRETRAIN_MODEL.MODEL_NAME}', 'EXPORTER_SCRIPT': '/opt/models/research/object_detection/exporter_main_v2.py', 'CONFIG_PIPELINE_PATH': '${OUTPUTS.OUPUT_DIR}/', 'CHECKPOINT_PATH': '${OUTPUTS.OUPUT_DIR}/checkpoints/'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.OUTPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATASET_DIRS = Path(cfg.DATASET.DATASET_DIR)\n",
    "DATASET_DIRS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_DIR = Path(cfg.DATASET_DIRS.TRAIN_DIR)\n",
    "VALIDATION_DIR = Path(cfg.DATASET_DIRS.VALIDATION_DIR)\n",
    "TEST_DIR = Path(cfg.DATASET_DIRS.TEST_DIR)\n",
    "\n",
    "IMG_SIZE = cfg.TRAIN.IMG_SIZE\n",
    "BATCH_SIZE = cfg.TRAIN.BATCH_SIZE\n",
    "\n",
    "TRAIN_ANNOT_FILE_PATH = TRAIN_DIR / cfg.DATASET.ANNOTATION_FILE_NAME\n",
    "TEST_ANNOT_FILE_PATH = TEST_DIR / cfg.DATASET.ANNOTATION_FILE_NAME\n",
    "VALID_ANNOT_FILE_PATH = VALIDATION_DIR / cfg.DATASET.ANNOTATION_FILE_NAME\n",
    "\n",
    "CONFIG_PIPELINE_PATH = Path(cfg.OUTPUTS.CONFIG_PIPELINE_PATH)\n",
    "PRETRAIN_MODEL_PATH  = Path(cfg.PRETRAIN_MODEL.PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('datasets/License-Plate-Recognition-8/train')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detections(image_np,\n",
    "                    boxes,\n",
    "                    classes,\n",
    "                    scores,\n",
    "                    category_index,\n",
    "                    figsize=(12, 16),\n",
    "                    image_name=None):\n",
    "    \"\"\"Wrapper function to visualize detections.\n",
    "\n",
    "    Args:\n",
    "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    boxes: a numpy array of shape [N, 4]\n",
    "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
    "          and match the keys in the label map.\n",
    "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
    "          this function assumes that the boxes to be plotted are groundtruth\n",
    "          boxes and plot all boxes as black with no classes or scores.\n",
    "    category_index: a dict containing category dictionaries (each holding\n",
    "          category index `id` and category name `name`) keyed by category indices.\n",
    "    figsize: size for the figure.\n",
    "    image_name: a name for the image file.\n",
    "    \"\"\"\n",
    "\n",
    "    image_np_with_annotations = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_annotations,\n",
    "        boxes,\n",
    "        classes,\n",
    "        scores,\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        min_score_thresh=0.8)\n",
    "\n",
    "    if image_name:\n",
    "        plt.imsave(image_name, image_np_with_annotations)\n",
    "\n",
    "    else:\n",
    "        plt.imshow(image_np_with_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(list(DATASET_DIRS.iterdir())) == 0:\n",
    "    from roboflow import Roboflow\n",
    "    rf = Roboflow()\n",
    "    project = rf.workspace(\"roboflow-universe-projects\").project(\"license-plate-recognition-rxg4e\")\n",
    "    version = project.version(8)\n",
    "    dataset = version.download(model_format=\"tensorflow\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 22, 22)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {\n",
    "    'License_Plate':1\n",
    "}\n",
    "\n",
    "prepare_train_dataset = AnnotationProcessor(annotation_file=TRAIN_ANNOT_FILE_PATH)\n",
    "\n",
    "train_images, train_class_ids, train_bboxes  = prepare_train_dataset.process_annotations(image_dir=TRAIN_DIR, label_map=label_map)\n",
    "# limit = 8\n",
    "# train_images = train_images[:limit]\n",
    "# train_bboxes = train_bboxes[:limit]\n",
    "# train_class_ids = train_class_ids[:limit]\n",
    "\n",
    "len(train_images), len(train_class_ids), len(train_bboxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.43229167, 0.80078125, 0.50260417, 0.890625  ],\n",
       "        [0.39713542, 0.96679688, 0.44270833, 0.99902344]]),\n",
       " array([[0.76010782, 0.42424242, 0.78975741, 0.47121212],\n",
       "        [0.54716981, 0.8       , 0.57412399, 0.82424242],\n",
       "        [0.55525606, 0.89848485, 0.58490566, 0.91818182],\n",
       "        [0.40431267, 0.59545455, 0.42587601, 0.61666667],\n",
       "        [0.35579515, 0.22878788, 0.38005391, 0.26969697],\n",
       "        [0.39622642, 0.39393939, 0.42048518, 0.42878788]]),\n",
       " array([[0.40924092, 0.66949153, 0.48844884, 0.79449153]]),\n",
       " array([[0.63036304, 0.47881356, 0.73267327, 0.69067797]]),\n",
       " array([[0.64453125, 0.09960938, 0.69661458, 0.14941406]]),\n",
       " array([[0.54571429, 0.24347826, 0.65142857, 0.32826087],\n",
       "        [0.33428571, 0.8173913 , 0.37142857, 0.87826087],\n",
       "        [0.33142857, 0.29565217, 0.36857143, 0.33695652],\n",
       "        [0.29142857, 0.58478261, 0.30285714, 0.61304348]]),\n",
       " array([[0.5709571 , 0.33262712, 0.67986799, 0.56567797]]),\n",
       " array([[0.5754717 , 0.553125  , 0.71462264, 0.66875   ]]),\n",
       " array([[0.54582485, 0.42290076, 0.7209776 , 0.66259542]]),\n",
       " array([[0.60377358, 0.094     , 0.80053908, 0.638     ]]),\n",
       " array([[0.58797654, 0.40625   , 0.67741935, 0.50976562],\n",
       "        [0.52199413, 0.7890625 , 0.55718475, 0.86523438]]),\n",
       " array([[0.57099698, 0.598     , 0.70694864, 0.72      ]]),\n",
       " array([[0.41584158, 0.49364407, 0.50165017, 0.66313559]]),\n",
       " array([[0.50390625, 0.76757812, 0.57161458, 0.84277344]]),\n",
       " array([[0.47882736, 0.48      , 0.57166124, 0.5825    ]]),\n",
       " array([[0.64026403, 0.38559322, 0.73927393, 0.58686441]]),\n",
       " array([[0.03102625, 0.02625   , 0.95942721, 0.9775    ]]),\n",
       " array([[0.46742671, 0.07910156, 0.50162866, 0.1171875 ],\n",
       "        [0.53583062, 0.45703125, 0.57654723, 0.52246094],\n",
       "        [0.44299674, 0.93359375, 0.46254072, 0.95410156]]),\n",
       " array([[0.50240385, 0.50480769, 0.58413462, 0.62740385]]),\n",
       " array([[0.36481481, 0.24166667, 0.43333333, 0.29166667],\n",
       "        [0.32407407, 0.55208333, 0.36666667, 0.57083333],\n",
       "        [0.4037037 , 0.73125   , 0.47222222, 0.784375  ]]),\n",
       " array([[0.87788779, 0.40254237, 0.99669967, 0.67372881]]),\n",
       " array([[0.21596244, 0.06161972, 0.84976526, 0.96302817]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 13, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_valid_dataset = AnnotationProcessor(annotation_file=VALID_ANNOT_FILE_PATH)\n",
    "\n",
    "valid_images, valid_class_ids, valid_bboxes  = prepare_valid_dataset.process_annotations(image_dir=VALIDATION_DIR, label_map=label_map)\n",
    "\n",
    "len(valid_images), len(valid_class_ids), len(valid_bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to increase training size and get better loss, combining validation dataset with training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 35, 35)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.extend(valid_images), train_class_ids.extend(valid_class_ids), train_bboxes.extend(valid_bboxes)\n",
    "len(train_images), len(train_class_ids), len(train_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15, 15)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_test_dataset = AnnotationProcessor(annotation_file=TEST_ANNOT_FILE_PATH)\n",
    "\n",
    "test_images, test_class_ids, test_bboxes  = prepare_test_dataset.process_annotations(image_dir=TEST_DIR, label_map=label_map)\n",
    "\n",
    "len(test_images), len(test_class_ids), len(test_bboxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the category index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'name': 'License_Plate'}\n"
     ]
    }
   ],
   "source": [
    "# Assign the license plate class ID\n",
    "class_id = 1\n",
    "\n",
    "# define a dictionary describing license plate class\n",
    "category_index = {class_id :\n",
    "{'id'  : class_id,\n",
    " 'name': 'License_Plate'}\n",
    "}\n",
    "\n",
    "# Specify the number of classes that the model will predict\n",
    "num_classes = 1\n",
    "print(category_index[class_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "- Convert the class labels to one-hot representations\n",
    "- convert everything (i.e. train images, gt boxes and class labels) to tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `label_id_offset` here shifts all classes by a certain number of indices;\n",
    "# we do this here so that the model receives one-hot labels where non-background\n",
    "# classes start counting at the zeroth index.  This is ordinarily just handled\n",
    "# automatically in our training binaries, but we need to reproduce it here.\n",
    "\n",
    "label_id_offset = 1\n",
    "train_image_tensors = []\n",
    "\n",
    "# lists containing the one-hot encoded classes and ground truth boxes\n",
    "gt_classes_one_hot_tensors = []\n",
    "gt_box_tensors = []\n",
    "\n",
    "for train_image, bbox in zip(train_images, train_bboxes):\n",
    "    # convert training image to tensor, add batch dimension, and add to list\n",
    "    train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(train_image, dtype=tf.float32), axis=0))\n",
    "    \n",
    "    # convert numpy array to tensor, then add to list\n",
    "    gt_box_tensors.append(tf.convert_to_tensor(bbox, dtype=tf.float32))\n",
    "\n",
    "    # apply offset to to have zero-indexed ground truth classes\n",
    "    zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
    "        np.ones(shape=[bbox.shape[0]], dtype=np.int32) - label_id_offset\n",
    "    )\n",
    "    # do one-hot encoding to ground truth classes\n",
    "    gt_classes_one_hot_tensors.append(tf.one_hot(zero_indexed_groundtruth_classes, num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the random license plate with their ground truth bounding boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random samples: [31, 12, 15, 20, 6, 33, 10, 23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_138325/2298106281.py:26: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# give boxes a score of 100%\n",
    "import random\n",
    "\n",
    "\n",
    "dummy_scores = np.array([1.0], dtype=np.float32)\n",
    "\n",
    "# define the figure size\n",
    "plt.figure(figsize=(30, 15))\n",
    "\n",
    "# use the `plot_detections()` utility function to draw the ground truth boxes\n",
    "random_samples = random.sample(range(len(train_images)), 8)\n",
    "print(f\"Random samples: {random_samples}\")\n",
    "\n",
    "for i, idx in enumerate(random_samples):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    dummy = np.ones(shape=[train_bboxes[idx].shape[0]], dtype=np.int32)\n",
    "    # print(train_images[idx])\n",
    "    plot_detections(\n",
    "      image_np=train_images[idx],\n",
    "     boxes=train_bboxes[idx],\n",
    "    classes=dummy,\n",
    "    scores=dummy, \n",
    "    category_index=category_index)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the checkpoint containing the pre-trained weights for [RetinaNet](https://arxiv.org/abs/1708.02002) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the SSD Resnet 50 version 1, 640x640 checkpoint\n",
    "if not Path('ssd_resnet50_v1_fpn_640x640_coco17_tpu-8').exists():\n",
    "    !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "\n",
    "    # untar (decompress) the tar file # -C /path/to/destination/folder\n",
    "    !tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "\n",
    "    # copy the checkpoint to the test_data folder models/research/object_detection/test_data/\n",
    "    !cp -r ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': ssd {\n",
       "   num_classes: 90\n",
       "   image_resizer {\n",
       "     fixed_shape_resizer {\n",
       "       height: 640\n",
       "       width: 640\n",
       "     }\n",
       "   }\n",
       "   feature_extractor {\n",
       "     type: \"ssd_resnet50_v1_fpn_keras\"\n",
       "     depth_multiplier: 1.0\n",
       "     min_depth: 16\n",
       "     conv_hyperparams {\n",
       "       regularizer {\n",
       "         l2_regularizer {\n",
       "           weight: 0.0004\n",
       "         }\n",
       "       }\n",
       "       initializer {\n",
       "         truncated_normal_initializer {\n",
       "           mean: 0.0\n",
       "           stddev: 0.03\n",
       "         }\n",
       "       }\n",
       "       activation: RELU_6\n",
       "       batch_norm {\n",
       "         decay: 0.997\n",
       "         scale: true\n",
       "         epsilon: 0.001\n",
       "       }\n",
       "     }\n",
       "     override_base_feature_extractor_hyperparams: true\n",
       "     fpn {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "     }\n",
       "   }\n",
       "   box_coder {\n",
       "     faster_rcnn_box_coder {\n",
       "       y_scale: 10.0\n",
       "       x_scale: 10.0\n",
       "       height_scale: 5.0\n",
       "       width_scale: 5.0\n",
       "     }\n",
       "   }\n",
       "   matcher {\n",
       "     argmax_matcher {\n",
       "       matched_threshold: 0.5\n",
       "       unmatched_threshold: 0.5\n",
       "       ignore_thresholds: false\n",
       "       negatives_lower_than_unmatched: true\n",
       "       force_match_for_each_row: true\n",
       "       use_matmul_gather: true\n",
       "     }\n",
       "   }\n",
       "   similarity_calculator {\n",
       "     iou_similarity {\n",
       "     }\n",
       "   }\n",
       "   box_predictor {\n",
       "     weight_shared_convolutional_box_predictor {\n",
       "       conv_hyperparams {\n",
       "         regularizer {\n",
       "           l2_regularizer {\n",
       "             weight: 0.0004\n",
       "           }\n",
       "         }\n",
       "         initializer {\n",
       "           random_normal_initializer {\n",
       "             mean: 0.0\n",
       "             stddev: 0.01\n",
       "           }\n",
       "         }\n",
       "         activation: RELU_6\n",
       "         batch_norm {\n",
       "           decay: 0.997\n",
       "           scale: true\n",
       "           epsilon: 0.001\n",
       "         }\n",
       "       }\n",
       "       depth: 256\n",
       "       num_layers_before_predictor: 4\n",
       "       kernel_size: 3\n",
       "       class_prediction_bias_init: -4.6\n",
       "     }\n",
       "   }\n",
       "   anchor_generator {\n",
       "     multiscale_anchor_generator {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       anchor_scale: 4.0\n",
       "       aspect_ratios: 1.0\n",
       "       aspect_ratios: 2.0\n",
       "       aspect_ratios: 0.5\n",
       "       scales_per_octave: 2\n",
       "     }\n",
       "   }\n",
       "   post_processing {\n",
       "     batch_non_max_suppression {\n",
       "       score_threshold: 1e-08\n",
       "       iou_threshold: 0.6\n",
       "       max_detections_per_class: 100\n",
       "       max_total_detections: 100\n",
       "     }\n",
       "     score_converter: SIGMOID\n",
       "   }\n",
       "   normalize_loss_by_num_matches: true\n",
       "   loss {\n",
       "     localization_loss {\n",
       "       weighted_smooth_l1 {\n",
       "       }\n",
       "     }\n",
       "     classification_loss {\n",
       "       weighted_sigmoid_focal {\n",
       "         gamma: 2.0\n",
       "         alpha: 0.25\n",
       "       }\n",
       "     }\n",
       "     classification_weight: 1.0\n",
       "     localization_weight: 1.0\n",
       "   }\n",
       "   encode_background_as_zeros: true\n",
       "   normalize_loc_loss_by_codesize: true\n",
       "   inplace_batchnorm_update: true\n",
       "   freeze_batchnorm: false\n",
       " },\n",
       " 'train_config': batch_size: 64\n",
       " data_augmentation_options {\n",
       "   random_horizontal_flip {\n",
       "   }\n",
       " }\n",
       " data_augmentation_options {\n",
       "   random_crop_image {\n",
       "     min_object_covered: 0.0\n",
       "     min_aspect_ratio: 0.75\n",
       "     max_aspect_ratio: 3.0\n",
       "     min_area: 0.75\n",
       "     max_area: 1.0\n",
       "     overlap_thresh: 0.0\n",
       "   }\n",
       " }\n",
       " sync_replicas: true\n",
       " optimizer {\n",
       "   momentum_optimizer {\n",
       "     learning_rate {\n",
       "       cosine_decay_learning_rate {\n",
       "         learning_rate_base: 0.04\n",
       "         total_steps: 25000\n",
       "         warmup_learning_rate: 0.013333\n",
       "         warmup_steps: 2000\n",
       "       }\n",
       "     }\n",
       "     momentum_optimizer_value: 0.9\n",
       "   }\n",
       "   use_moving_average: false\n",
       " }\n",
       " fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/resnet50.ckpt-1\"\n",
       " num_steps: 25000\n",
       " startup_delay_steps: 0.0\n",
       " replicas_to_aggregate: 8\n",
       " max_number_of_boxes: 100\n",
       " unpad_groundtruth_tensors: false\n",
       " fine_tune_checkpoint_type: \"classification\"\n",
       " use_bfloat16: true\n",
       " fine_tune_checkpoint_version: V2,\n",
       " 'train_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED/label_map.txt\"\n",
       " tf_record_input_reader {\n",
       "   input_path: \"PATH_TO_BE_CONFIGURED/train2017-?????-of-00256.tfrecord\"\n",
       " },\n",
       " 'eval_config': metrics_set: \"coco_detection_metrics\"\n",
       " use_moving_averages: false,\n",
       " 'eval_input_configs': [label_map_path: \"PATH_TO_BE_CONFIGURED/label_map.txt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"PATH_TO_BE_CONFIGURED/val2017-?????-of-00032.tfrecord\"\n",
       " }\n",
       " ],\n",
       " 'eval_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED/label_map.txt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"PATH_TO_BE_CONFIGURED/val2017-?????-of-00032.tfrecord\"\n",
       " }}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "### START CODE HERE ###\n",
    "# define the path to the .config file for ssd resnet 50 v1 640x640\n",
    "pipeline_config = '/opt/models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "\n",
    "# Load the configuration file into a dictionary\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "\n",
    "### END CODE HERE ###\n",
    "# See what configs looks like\n",
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ssd {\n",
       "  num_classes: 90\n",
       "  image_resizer {\n",
       "    fixed_shape_resizer {\n",
       "      height: 640\n",
       "      width: 640\n",
       "    }\n",
       "  }\n",
       "  feature_extractor {\n",
       "    type: \"ssd_resnet50_v1_fpn_keras\"\n",
       "    depth_multiplier: 1.0\n",
       "    min_depth: 16\n",
       "    conv_hyperparams {\n",
       "      regularizer {\n",
       "        l2_regularizer {\n",
       "          weight: 0.0004\n",
       "        }\n",
       "      }\n",
       "      initializer {\n",
       "        truncated_normal_initializer {\n",
       "          mean: 0.0\n",
       "          stddev: 0.03\n",
       "        }\n",
       "      }\n",
       "      activation: RELU_6\n",
       "      batch_norm {\n",
       "        decay: 0.997\n",
       "        scale: true\n",
       "        epsilon: 0.001\n",
       "      }\n",
       "    }\n",
       "    override_base_feature_extractor_hyperparams: true\n",
       "    fpn {\n",
       "      min_level: 3\n",
       "      max_level: 7\n",
       "    }\n",
       "  }\n",
       "  box_coder {\n",
       "    faster_rcnn_box_coder {\n",
       "      y_scale: 10.0\n",
       "      x_scale: 10.0\n",
       "      height_scale: 5.0\n",
       "      width_scale: 5.0\n",
       "    }\n",
       "  }\n",
       "  matcher {\n",
       "    argmax_matcher {\n",
       "      matched_threshold: 0.5\n",
       "      unmatched_threshold: 0.5\n",
       "      ignore_thresholds: false\n",
       "      negatives_lower_than_unmatched: true\n",
       "      force_match_for_each_row: true\n",
       "      use_matmul_gather: true\n",
       "    }\n",
       "  }\n",
       "  similarity_calculator {\n",
       "    iou_similarity {\n",
       "    }\n",
       "  }\n",
       "  box_predictor {\n",
       "    weight_shared_convolutional_box_predictor {\n",
       "      conv_hyperparams {\n",
       "        regularizer {\n",
       "          l2_regularizer {\n",
       "            weight: 0.0004\n",
       "          }\n",
       "        }\n",
       "        initializer {\n",
       "          random_normal_initializer {\n",
       "            mean: 0.0\n",
       "            stddev: 0.01\n",
       "          }\n",
       "        }\n",
       "        activation: RELU_6\n",
       "        batch_norm {\n",
       "          decay: 0.997\n",
       "          scale: true\n",
       "          epsilon: 0.001\n",
       "        }\n",
       "      }\n",
       "      depth: 256\n",
       "      num_layers_before_predictor: 4\n",
       "      kernel_size: 3\n",
       "      class_prediction_bias_init: -4.6\n",
       "    }\n",
       "  }\n",
       "  anchor_generator {\n",
       "    multiscale_anchor_generator {\n",
       "      min_level: 3\n",
       "      max_level: 7\n",
       "      anchor_scale: 4.0\n",
       "      aspect_ratios: 1.0\n",
       "      aspect_ratios: 2.0\n",
       "      aspect_ratios: 0.5\n",
       "      scales_per_octave: 2\n",
       "    }\n",
       "  }\n",
       "  post_processing {\n",
       "    batch_non_max_suppression {\n",
       "      score_threshold: 1e-08\n",
       "      iou_threshold: 0.6\n",
       "      max_detections_per_class: 100\n",
       "      max_total_detections: 100\n",
       "    }\n",
       "    score_converter: SIGMOID\n",
       "  }\n",
       "  normalize_loss_by_num_matches: true\n",
       "  loss {\n",
       "    localization_loss {\n",
       "      weighted_smooth_l1 {\n",
       "      }\n",
       "    }\n",
       "    classification_loss {\n",
       "      weighted_sigmoid_focal {\n",
       "        gamma: 2.0\n",
       "        alpha: 0.25\n",
       "      }\n",
       "    }\n",
       "    classification_weight: 1.0\n",
       "    localization_weight: 1.0\n",
       "  }\n",
       "  encode_background_as_zeros: true\n",
       "  normalize_loc_loss_by_codesize: true\n",
       "  inplace_batchnorm_update: true\n",
       "  freeze_batchnorm: false\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the object stored at the key 'model' of the configs dictionary\n",
    "model_config = configs['model']\n",
    "\n",
    "### END CODE HERE\n",
    "# see what model_config looks like\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ssd {\n",
       "  num_classes: 1\n",
       "  image_resizer {\n",
       "    fixed_shape_resizer {\n",
       "      height: 640\n",
       "      width: 640\n",
       "    }\n",
       "  }\n",
       "  feature_extractor {\n",
       "    type: \"ssd_resnet50_v1_fpn_keras\"\n",
       "    depth_multiplier: 1.0\n",
       "    min_depth: 16\n",
       "    conv_hyperparams {\n",
       "      regularizer {\n",
       "        l2_regularizer {\n",
       "          weight: 0.0004\n",
       "        }\n",
       "      }\n",
       "      initializer {\n",
       "        truncated_normal_initializer {\n",
       "          mean: 0.0\n",
       "          stddev: 0.03\n",
       "        }\n",
       "      }\n",
       "      activation: RELU_6\n",
       "      batch_norm {\n",
       "        decay: 0.997\n",
       "        scale: true\n",
       "        epsilon: 0.001\n",
       "      }\n",
       "    }\n",
       "    override_base_feature_extractor_hyperparams: true\n",
       "    fpn {\n",
       "      min_level: 3\n",
       "      max_level: 7\n",
       "    }\n",
       "  }\n",
       "  box_coder {\n",
       "    faster_rcnn_box_coder {\n",
       "      y_scale: 10.0\n",
       "      x_scale: 10.0\n",
       "      height_scale: 5.0\n",
       "      width_scale: 5.0\n",
       "    }\n",
       "  }\n",
       "  matcher {\n",
       "    argmax_matcher {\n",
       "      matched_threshold: 0.5\n",
       "      unmatched_threshold: 0.5\n",
       "      ignore_thresholds: false\n",
       "      negatives_lower_than_unmatched: true\n",
       "      force_match_for_each_row: true\n",
       "      use_matmul_gather: true\n",
       "    }\n",
       "  }\n",
       "  similarity_calculator {\n",
       "    iou_similarity {\n",
       "    }\n",
       "  }\n",
       "  box_predictor {\n",
       "    weight_shared_convolutional_box_predictor {\n",
       "      conv_hyperparams {\n",
       "        regularizer {\n",
       "          l2_regularizer {\n",
       "            weight: 0.0004\n",
       "          }\n",
       "        }\n",
       "        initializer {\n",
       "          random_normal_initializer {\n",
       "            mean: 0.0\n",
       "            stddev: 0.01\n",
       "          }\n",
       "        }\n",
       "        activation: RELU_6\n",
       "        batch_norm {\n",
       "          decay: 0.997\n",
       "          scale: true\n",
       "          epsilon: 0.001\n",
       "        }\n",
       "      }\n",
       "      depth: 256\n",
       "      num_layers_before_predictor: 4\n",
       "      kernel_size: 3\n",
       "      class_prediction_bias_init: -4.6\n",
       "    }\n",
       "  }\n",
       "  anchor_generator {\n",
       "    multiscale_anchor_generator {\n",
       "      min_level: 3\n",
       "      max_level: 7\n",
       "      anchor_scale: 4.0\n",
       "      aspect_ratios: 1.0\n",
       "      aspect_ratios: 2.0\n",
       "      aspect_ratios: 0.5\n",
       "      scales_per_octave: 2\n",
       "    }\n",
       "  }\n",
       "  post_processing {\n",
       "    batch_non_max_suppression {\n",
       "      score_threshold: 1e-08\n",
       "      iou_threshold: 0.6\n",
       "      max_detections_per_class: 100\n",
       "      max_total_detections: 100\n",
       "    }\n",
       "    score_converter: SIGMOID\n",
       "  }\n",
       "  normalize_loss_by_num_matches: true\n",
       "  loss {\n",
       "    localization_loss {\n",
       "      weighted_smooth_l1 {\n",
       "      }\n",
       "    }\n",
       "    classification_loss {\n",
       "      weighted_sigmoid_focal {\n",
       "        gamma: 2.0\n",
       "        alpha: 0.25\n",
       "      }\n",
       "    }\n",
       "    classification_weight: 1.0\n",
       "    localization_weight: 1.0\n",
       "  }\n",
       "  encode_background_as_zeros: true\n",
       "  normalize_loc_loss_by_codesize: true\n",
       "  inplace_batchnorm_update: true\n",
       "  freeze_batchnorm: true\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify the number of classes from its default of 90\n",
    "model_config.ssd.num_classes = num_classes\n",
    "\n",
    "# Freeze batch normalization\n",
    "model_config.ssd.freeze_batchnorm = True\n",
    "\n",
    "### END CODE HERE\n",
    "\n",
    "# See what model_config now looks like after you've customized it!\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exported_models/my_ssd_resnet50_v1_fpn_640x640_coco17_tpu-8'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(CONFIG_PIPELINE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_util.save_pipeline_config(pipeline_config=model_config, directory=str(CONFIG_PIPELINE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_builder.build(model_config=model_config,is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_box_predictor_checkpoint = tf.train.Checkpoint(\n",
    "   _base_tower_layers_for_heads = model._box_predictor._base_tower_layers_for_heads,\n",
    "   _box_prediction_head = model._box_predictor._box_prediction_head\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the temporary model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x77a6e9ca27d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model_checkpoint = tf.compat.v2.train.Checkpoint (\n",
    "    _feature_extractor=model._feature_extractor,\n",
    "    _box_predictor = tmp_box_predictor_checkpoint\n",
    ")\n",
    "checkpoint_path = 'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0'\n",
    "\n",
    "# Define a checkpoint that sets `model` to the temporary model checkpoint\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    model=tmp_model_checkpoint\n",
    ")\n",
    "\n",
    "# Restore the checkpoint to the checkpoint path\n",
    "checkpoint.restore(save_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a dummy image to generate the model variables\n",
    "# use the detection model's `preprocess()` method and pass a dummy image\n",
    "dummy_img = tf.zeros([1,640,640,3])\n",
    "tmp_image, tmp_shapes = model.preprocess(dummy_img)\n",
    "\n",
    "# run a prediction with the preprocessed image and shapes\n",
    "tmp_prediction_dict = model.predict(tmp_image, tmp_shapes)\n",
    "\n",
    "# postprocess the predictions into final detections\n",
    "tmp_detections = model.postprocess(tmp_prediction_dict, tmp_shapes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.venv/lib/python3.10/site-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.set_learning_phase(True)\n",
    "\n",
    "# set the batch_size\n",
    "batch_size = 4\n",
    "\n",
    "# set the number of batches\n",
    "EPOCHS = 200\n",
    "\n",
    "# Set the learning rate\n",
    "# learning_rate =0.008 #0.0001#0.0008\n",
    "\n",
    "# # set the optimizer and pass in the learning_rate\n",
    "initial_learning_rate = 0.004\n",
    "decay_steps = 50\n",
    "decay_rate = 0.96#0.96\n",
    "\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps,\n",
    "    decay_rate,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate, momentum=0.9)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate, use_ema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the layers to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 \t name: WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/kernel:0 \t shape:(3, 3, 256, 24) \t dtype=<dtype: 'float32'>\n",
      "i: 1 \t name: WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/bias:0 \t shape:(24,) \t dtype=<dtype: 'float32'>\n",
      "i: 2 \t name: WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/kernel:0 \t shape:(3, 3, 256, 12) \t dtype=<dtype: 'float32'>\n",
      "i: 3 \t name: WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/bias:0 \t shape:(12,) \t dtype=<dtype: 'float32'>\n",
      "i: 4 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 5 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_0/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 6 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_0/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 7 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 8 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_0/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 9 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_0/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 10 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 11 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_0/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 12 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_0/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 13 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 14 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_0/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 15 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_0/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 16 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_1/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 17 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_1/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 18 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_1/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 19 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_1/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 20 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_1/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 21 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_1/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 22 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_1/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 23 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_1/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 24 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_2/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 25 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_2/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 26 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_2/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 27 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_2/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 28 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_2/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 29 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_2/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 30 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_2/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 31 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_2/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 32 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_3/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 33 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_3/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 34 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_3/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 35 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_3/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 36 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_3/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 37 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_3/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 38 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_3/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 39 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_3/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 40 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_4/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 41 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm/feature_4/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 42 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_4/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 43 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm/feature_4/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 44 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_4/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 45 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm/feature_4/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 46 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_4/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 47 \t name: WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm/feature_4/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 48 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 49 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_0/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 50 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_0/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 51 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 52 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_0/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 53 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_0/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 54 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 55 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_0/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 56 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_0/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 57 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 58 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_0/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 59 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_0/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 60 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_1/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 61 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_1/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 62 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_1/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 63 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_1/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 64 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_1/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 65 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_1/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 66 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_1/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 67 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_1/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 68 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_2/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 69 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_2/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 70 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_2/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 71 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_2/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 72 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_2/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 73 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_2/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 74 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_2/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 75 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_2/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 76 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_3/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 77 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_3/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 78 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_3/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 79 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_3/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 80 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_3/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 81 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_3/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 82 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_3/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 83 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_3/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 84 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_4/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 85 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm/feature_4/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 86 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_4/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 87 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm/feature_4/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 88 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_4/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 89 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm/feature_4/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 90 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_4/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 91 \t name: WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm/feature_4/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 92 \t name: ResNet50V1_FPN/bottom_up_block5_conv/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 93 \t name: ResNet50V1_FPN/bottom_up_block5_batchnorm/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 94 \t name: ResNet50V1_FPN/bottom_up_block5_batchnorm/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 95 \t name: ResNet50V1_FPN/bottom_up_block6_conv/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 96 \t name: ResNet50V1_FPN/bottom_up_block6_batchnorm/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 97 \t name: ResNet50V1_FPN/bottom_up_block6_batchnorm/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 98 \t name: conv1_conv/kernel:0 \t shape:(7, 7, 3, 64) \t dtype=<dtype: 'float32'>\n",
      "i: 99 \t name: conv1_bn/gamma:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 100 \t name: conv1_bn/beta:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 101 \t name: conv2_block1_1_conv/kernel:0 \t shape:(1, 1, 64, 64) \t dtype=<dtype: 'float32'>\n",
      "i: 102 \t name: conv2_block1_1_bn/gamma:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 103 \t name: conv2_block1_1_bn/beta:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 104 \t name: conv2_block1_2_conv/kernel:0 \t shape:(3, 3, 64, 64) \t dtype=<dtype: 'float32'>\n",
      "i: 105 \t name: conv2_block1_2_bn/gamma:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 106 \t name: conv2_block1_2_bn/beta:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 107 \t name: conv2_block1_0_conv/kernel:0 \t shape:(1, 1, 64, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 108 \t name: conv2_block1_3_conv/kernel:0 \t shape:(1, 1, 64, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 109 \t name: conv2_block1_0_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 110 \t name: conv2_block1_0_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 111 \t name: conv2_block1_3_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 112 \t name: conv2_block1_3_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 113 \t name: conv2_block2_1_conv/kernel:0 \t shape:(1, 1, 256, 64) \t dtype=<dtype: 'float32'>\n",
      "i: 114 \t name: conv2_block2_1_bn/gamma:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 115 \t name: conv2_block2_1_bn/beta:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 116 \t name: conv2_block2_2_conv/kernel:0 \t shape:(3, 3, 64, 64) \t dtype=<dtype: 'float32'>\n",
      "i: 117 \t name: conv2_block2_2_bn/gamma:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 118 \t name: conv2_block2_2_bn/beta:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 119 \t name: conv2_block2_3_conv/kernel:0 \t shape:(1, 1, 64, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 120 \t name: conv2_block2_3_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 121 \t name: conv2_block2_3_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 122 \t name: conv2_block3_1_conv/kernel:0 \t shape:(1, 1, 256, 64) \t dtype=<dtype: 'float32'>\n",
      "i: 123 \t name: conv2_block3_1_bn/gamma:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 124 \t name: conv2_block3_1_bn/beta:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 125 \t name: conv2_block3_2_conv/kernel:0 \t shape:(3, 3, 64, 64) \t dtype=<dtype: 'float32'>\n",
      "i: 126 \t name: conv2_block3_2_bn/gamma:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 127 \t name: conv2_block3_2_bn/beta:0 \t shape:(64,) \t dtype=<dtype: 'float32'>\n",
      "i: 128 \t name: conv2_block3_3_conv/kernel:0 \t shape:(1, 1, 64, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 129 \t name: conv2_block3_3_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 130 \t name: conv2_block3_3_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 131 \t name: conv3_block1_1_conv/kernel:0 \t shape:(1, 1, 256, 128) \t dtype=<dtype: 'float32'>\n",
      "i: 132 \t name: conv3_block1_1_bn/gamma:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 133 \t name: conv3_block1_1_bn/beta:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 134 \t name: conv3_block1_2_conv/kernel:0 \t shape:(3, 3, 128, 128) \t dtype=<dtype: 'float32'>\n",
      "i: 135 \t name: conv3_block1_2_bn/gamma:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 136 \t name: conv3_block1_2_bn/beta:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 137 \t name: conv3_block1_0_conv/kernel:0 \t shape:(1, 1, 256, 512) \t dtype=<dtype: 'float32'>\n",
      "i: 138 \t name: conv3_block1_3_conv/kernel:0 \t shape:(1, 1, 128, 512) \t dtype=<dtype: 'float32'>\n",
      "i: 139 \t name: conv3_block1_0_bn/gamma:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 140 \t name: conv3_block1_0_bn/beta:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 141 \t name: conv3_block1_3_bn/gamma:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 142 \t name: conv3_block1_3_bn/beta:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 143 \t name: conv3_block2_1_conv/kernel:0 \t shape:(1, 1, 512, 128) \t dtype=<dtype: 'float32'>\n",
      "i: 144 \t name: conv3_block2_1_bn/gamma:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 145 \t name: conv3_block2_1_bn/beta:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 146 \t name: conv3_block2_2_conv/kernel:0 \t shape:(3, 3, 128, 128) \t dtype=<dtype: 'float32'>\n",
      "i: 147 \t name: conv3_block2_2_bn/gamma:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 148 \t name: conv3_block2_2_bn/beta:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 149 \t name: conv3_block2_3_conv/kernel:0 \t shape:(1, 1, 128, 512) \t dtype=<dtype: 'float32'>\n",
      "i: 150 \t name: conv3_block2_3_bn/gamma:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 151 \t name: conv3_block2_3_bn/beta:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 152 \t name: conv3_block3_1_conv/kernel:0 \t shape:(1, 1, 512, 128) \t dtype=<dtype: 'float32'>\n",
      "i: 153 \t name: conv3_block3_1_bn/gamma:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 154 \t name: conv3_block3_1_bn/beta:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 155 \t name: conv3_block3_2_conv/kernel:0 \t shape:(3, 3, 128, 128) \t dtype=<dtype: 'float32'>\n",
      "i: 156 \t name: conv3_block3_2_bn/gamma:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 157 \t name: conv3_block3_2_bn/beta:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 158 \t name: conv3_block3_3_conv/kernel:0 \t shape:(1, 1, 128, 512) \t dtype=<dtype: 'float32'>\n",
      "i: 159 \t name: conv3_block3_3_bn/gamma:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 160 \t name: conv3_block3_3_bn/beta:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 161 \t name: conv3_block4_1_conv/kernel:0 \t shape:(1, 1, 512, 128) \t dtype=<dtype: 'float32'>\n",
      "i: 162 \t name: conv3_block4_1_bn/gamma:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 163 \t name: conv3_block4_1_bn/beta:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 164 \t name: conv3_block4_2_conv/kernel:0 \t shape:(3, 3, 128, 128) \t dtype=<dtype: 'float32'>\n",
      "i: 165 \t name: conv3_block4_2_bn/gamma:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 166 \t name: conv3_block4_2_bn/beta:0 \t shape:(128,) \t dtype=<dtype: 'float32'>\n",
      "i: 167 \t name: conv3_block4_3_conv/kernel:0 \t shape:(1, 1, 128, 512) \t dtype=<dtype: 'float32'>\n",
      "i: 168 \t name: conv3_block4_3_bn/gamma:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 169 \t name: conv3_block4_3_bn/beta:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 170 \t name: conv4_block1_1_conv/kernel:0 \t shape:(1, 1, 512, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 171 \t name: conv4_block1_1_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 172 \t name: conv4_block1_1_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 173 \t name: conv4_block1_2_conv/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 174 \t name: conv4_block1_2_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 175 \t name: conv4_block1_2_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 176 \t name: conv4_block1_0_conv/kernel:0 \t shape:(1, 1, 512, 1024) \t dtype=<dtype: 'float32'>\n",
      "i: 177 \t name: conv4_block1_3_conv/kernel:0 \t shape:(1, 1, 256, 1024) \t dtype=<dtype: 'float32'>\n",
      "i: 178 \t name: conv4_block1_0_bn/gamma:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 179 \t name: conv4_block1_0_bn/beta:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 180 \t name: conv4_block1_3_bn/gamma:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 181 \t name: conv4_block1_3_bn/beta:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 182 \t name: conv4_block2_1_conv/kernel:0 \t shape:(1, 1, 1024, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 183 \t name: conv4_block2_1_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 184 \t name: conv4_block2_1_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 185 \t name: conv4_block2_2_conv/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 186 \t name: conv4_block2_2_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 187 \t name: conv4_block2_2_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 188 \t name: conv4_block2_3_conv/kernel:0 \t shape:(1, 1, 256, 1024) \t dtype=<dtype: 'float32'>\n",
      "i: 189 \t name: conv4_block2_3_bn/gamma:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 190 \t name: conv4_block2_3_bn/beta:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 191 \t name: conv4_block3_1_conv/kernel:0 \t shape:(1, 1, 1024, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 192 \t name: conv4_block3_1_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 193 \t name: conv4_block3_1_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 194 \t name: conv4_block3_2_conv/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 195 \t name: conv4_block3_2_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 196 \t name: conv4_block3_2_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 197 \t name: conv4_block3_3_conv/kernel:0 \t shape:(1, 1, 256, 1024) \t dtype=<dtype: 'float32'>\n",
      "i: 198 \t name: conv4_block3_3_bn/gamma:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 199 \t name: conv4_block3_3_bn/beta:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 200 \t name: conv4_block4_1_conv/kernel:0 \t shape:(1, 1, 1024, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 201 \t name: conv4_block4_1_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 202 \t name: conv4_block4_1_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 203 \t name: conv4_block4_2_conv/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 204 \t name: conv4_block4_2_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 205 \t name: conv4_block4_2_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 206 \t name: conv4_block4_3_conv/kernel:0 \t shape:(1, 1, 256, 1024) \t dtype=<dtype: 'float32'>\n",
      "i: 207 \t name: conv4_block4_3_bn/gamma:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 208 \t name: conv4_block4_3_bn/beta:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 209 \t name: conv4_block5_1_conv/kernel:0 \t shape:(1, 1, 1024, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 210 \t name: conv4_block5_1_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 211 \t name: conv4_block5_1_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 212 \t name: conv4_block5_2_conv/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 213 \t name: conv4_block5_2_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 214 \t name: conv4_block5_2_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 215 \t name: conv4_block5_3_conv/kernel:0 \t shape:(1, 1, 256, 1024) \t dtype=<dtype: 'float32'>\n",
      "i: 216 \t name: conv4_block5_3_bn/gamma:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 217 \t name: conv4_block5_3_bn/beta:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 218 \t name: conv4_block6_1_conv/kernel:0 \t shape:(1, 1, 1024, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 219 \t name: conv4_block6_1_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 220 \t name: conv4_block6_1_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 221 \t name: conv4_block6_2_conv/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 222 \t name: conv4_block6_2_bn/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 223 \t name: conv4_block6_2_bn/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 224 \t name: conv4_block6_3_conv/kernel:0 \t shape:(1, 1, 256, 1024) \t dtype=<dtype: 'float32'>\n",
      "i: 225 \t name: conv4_block6_3_bn/gamma:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 226 \t name: conv4_block6_3_bn/beta:0 \t shape:(1024,) \t dtype=<dtype: 'float32'>\n",
      "i: 227 \t name: conv5_block1_1_conv/kernel:0 \t shape:(1, 1, 1024, 512) \t dtype=<dtype: 'float32'>\n",
      "i: 228 \t name: conv5_block1_1_bn/gamma:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 229 \t name: conv5_block1_1_bn/beta:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 230 \t name: conv5_block1_2_conv/kernel:0 \t shape:(3, 3, 512, 512) \t dtype=<dtype: 'float32'>\n",
      "i: 231 \t name: conv5_block1_2_bn/gamma:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 232 \t name: conv5_block1_2_bn/beta:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 233 \t name: conv5_block1_0_conv/kernel:0 \t shape:(1, 1, 1024, 2048) \t dtype=<dtype: 'float32'>\n",
      "i: 234 \t name: conv5_block1_3_conv/kernel:0 \t shape:(1, 1, 512, 2048) \t dtype=<dtype: 'float32'>\n",
      "i: 235 \t name: conv5_block1_0_bn/gamma:0 \t shape:(2048,) \t dtype=<dtype: 'float32'>\n",
      "i: 236 \t name: conv5_block1_0_bn/beta:0 \t shape:(2048,) \t dtype=<dtype: 'float32'>\n",
      "i: 237 \t name: conv5_block1_3_bn/gamma:0 \t shape:(2048,) \t dtype=<dtype: 'float32'>\n",
      "i: 238 \t name: conv5_block1_3_bn/beta:0 \t shape:(2048,) \t dtype=<dtype: 'float32'>\n",
      "i: 239 \t name: conv5_block2_1_conv/kernel:0 \t shape:(1, 1, 2048, 512) \t dtype=<dtype: 'float32'>\n",
      "i: 240 \t name: conv5_block2_1_bn/gamma:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 241 \t name: conv5_block2_1_bn/beta:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 242 \t name: conv5_block2_2_conv/kernel:0 \t shape:(3, 3, 512, 512) \t dtype=<dtype: 'float32'>\n",
      "i: 243 \t name: conv5_block2_2_bn/gamma:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 244 \t name: conv5_block2_2_bn/beta:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 245 \t name: conv5_block2_3_conv/kernel:0 \t shape:(1, 1, 512, 2048) \t dtype=<dtype: 'float32'>\n",
      "i: 246 \t name: conv5_block2_3_bn/gamma:0 \t shape:(2048,) \t dtype=<dtype: 'float32'>\n",
      "i: 247 \t name: conv5_block2_3_bn/beta:0 \t shape:(2048,) \t dtype=<dtype: 'float32'>\n",
      "i: 248 \t name: conv5_block3_1_conv/kernel:0 \t shape:(1, 1, 2048, 512) \t dtype=<dtype: 'float32'>\n",
      "i: 249 \t name: conv5_block3_1_bn/gamma:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 250 \t name: conv5_block3_1_bn/beta:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 251 \t name: conv5_block3_2_conv/kernel:0 \t shape:(3, 3, 512, 512) \t dtype=<dtype: 'float32'>\n",
      "i: 252 \t name: conv5_block3_2_bn/gamma:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 253 \t name: conv5_block3_2_bn/beta:0 \t shape:(512,) \t dtype=<dtype: 'float32'>\n",
      "i: 254 \t name: conv5_block3_3_conv/kernel:0 \t shape:(1, 1, 512, 2048) \t dtype=<dtype: 'float32'>\n",
      "i: 255 \t name: conv5_block3_3_bn/gamma:0 \t shape:(2048,) \t dtype=<dtype: 'float32'>\n",
      "i: 256 \t name: conv5_block3_3_bn/beta:0 \t shape:(2048,) \t dtype=<dtype: 'float32'>\n",
      "i: 257 \t name: ResNet50V1_FPN/FeatureMaps/top_down/projection_3/kernel:0 \t shape:(1, 1, 2048, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 258 \t name: ResNet50V1_FPN/FeatureMaps/top_down/projection_3/bias:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 259 \t name: ResNet50V1_FPN/FeatureMaps/top_down/projection_2/kernel:0 \t shape:(1, 1, 1024, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 260 \t name: ResNet50V1_FPN/FeatureMaps/top_down/projection_2/bias:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 261 \t name: ResNet50V1_FPN/FeatureMaps/top_down/projection_1/kernel:0 \t shape:(1, 1, 512, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 262 \t name: ResNet50V1_FPN/FeatureMaps/top_down/projection_1/bias:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 263 \t name: ResNet50V1_FPN/FeatureMaps/top_down/smoothing_2_conv/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 264 \t name: ResNet50V1_FPN/FeatureMaps/top_down/smoothing_2_batchnorm/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 265 \t name: ResNet50V1_FPN/FeatureMaps/top_down/smoothing_2_batchnorm/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 266 \t name: ResNet50V1_FPN/FeatureMaps/top_down/smoothing_1_conv/kernel:0 \t shape:(3, 3, 256, 256) \t dtype=<dtype: 'float32'>\n",
      "i: 267 \t name: ResNet50V1_FPN/FeatureMaps/top_down/smoothing_1_batchnorm/gamma:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n",
      "i: 268 \t name: ResNet50V1_FPN/FeatureMaps/top_down/smoothing_1_batchnorm/beta:0 \t shape:(256,) \t dtype=<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "# Inspect the layers of detection_model\n",
    "for i,v in enumerate(model.trainable_variables):\n",
    "    print(f\"i: {i} \\t name: {v.name} \\t shape:{v.shape} \\t dtype={v.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/kernel:0\n",
      "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/kernel:0\n"
     ]
    }
   ],
   "source": [
    "# Select the prediction layer variables\n",
    "\n",
    "# define a list that contains the layers that you wish to fine tune\n",
    "to_fine_tune = [\n",
    "model.trainable_variables[0],\n",
    "model.trainable_variables[1],\n",
    "model.trainable_variables[2],\n",
    "model.trainable_variables[3],\n",
    "model.trainable_variables[4],\n",
    "]\n",
    "print(to_fine_tune[0].name)\n",
    "print(to_fine_tune[2].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_preprocessed_image type: <class 'tuple'>\n",
      "g_preprocessed_image length: 2\n",
      "index 0 has the preprocessed image of shape (1, 640, 640, 3)\n",
      "index 1 has information about the image's true shape excluding padding: [[640 640   3]]\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of your training images\n",
    "g_images_list = train_image_tensors[0:2]\n",
    "\n",
    "# Use .preprocess to preprocess an image\n",
    "g_preprocessed_image = model.preprocess(g_images_list[0])\n",
    "print(f\"g_preprocessed_image type: {type(g_preprocessed_image)}\")\n",
    "print(f\"g_preprocessed_image length: {len(g_preprocessed_image)}\")\n",
    "print(f\"index 0 has the preprocessed image of shape {g_preprocessed_image[0].shape}\")\n",
    "print(f\"index 1 has information about the image's true shape excluding padding: {g_preprocessed_image[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre-process each image and save their outputs into two separate lists\n",
    "- One list of the preprocessed images\n",
    "- One list of the true shape for each preprocessed ima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_image_list is of type <class 'list'>\n",
      "preprocessed_image_list has length 2\n",
      "\n",
      "true_shape_list is of type <class 'list'>\n",
      "true_shape_list has length 2\n"
     ]
    }
   ],
   "source": [
    "preprocessed_image_list = []\n",
    "true_shape_list = []\n",
    "\n",
    "for img in g_images_list:\n",
    "    processed_img, true_shape = model.preprocess(img)\n",
    "    preprocessed_image_list.append(processed_img)\n",
    "    true_shape_list.append(true_shape)\n",
    "\n",
    "print(f\"preprocessed_image_list is of type {type(preprocessed_image_list)}\")\n",
    "print(f\"preprocessed_image_list has length {len(preprocessed_image_list)}\")\n",
    "print()\n",
    "print(f\"true_shape_list is of type {type(true_shape_list)}\")\n",
    "print(f\"true_shape_list has length {len(true_shape_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_image_tensor shape: (2, 640, 640, 3)\n",
      "true_shape_tensor shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Turn a list of tensors into a tensor\n",
    "preprocessed_image_tensor = tf.concat(preprocessed_image_list, axis=0)\n",
    "true_shape_tensor = tf.concat(true_shape_list, axis=0)\n",
    "\n",
    "print(f\"preprocessed_image_tensor shape: {preprocessed_image_tensor.shape}\")\n",
    "print(f\"true_shape_tensor shape: {true_shape_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys in prediction_dict:\n",
      "preprocessed_inputs\n",
      "feature_maps\n",
      "anchors\n",
      "final_anchors\n",
      "box_encodings\n",
      "class_predictions_with_background\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the images\n",
    "prediction_dict = model.predict(preprocessed_image_tensor, true_shape_tensor)\n",
    "\n",
    "print(\"keys in prediction_dict:\")\n",
    "for key in prediction_dict.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate loss\n",
    "```\n",
    "def loss(self, prediction_dict, true_image_shapes, scope=None):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss dictionary keys: dict_keys(['Loss/localization_loss', 'Loss/classification_loss'])\n",
      "localization loss 0.37901905\n",
      "classification loss 1.10095537\n"
     ]
    }
   ],
   "source": [
    "# Get the ground truth bounding boxes\n",
    "gt_boxes_list = gt_box_tensors[0:2]\n",
    "\n",
    "# Get the ground truth class labels\n",
    "gt_classes_list = gt_classes_one_hot_tensors[0:2]\n",
    "\n",
    "# Provide the ground truth to the model\n",
    "model.provide_groundtruth(\n",
    "            groundtruth_boxes_list=gt_boxes_list,\n",
    "            groundtruth_classes_list=gt_classes_list)\n",
    "\n",
    "# Calculate the loss after you've provided the ground truth\n",
    "losses_dict = model.loss(prediction_dict, true_shape_tensor)\n",
    "\n",
    "# View the loss dictionary\n",
    "losses_dict = model.loss(prediction_dict, true_shape_tensor)\n",
    "print(f\"loss dictionary keys: {losses_dict.keys()}\")\n",
    "print(f\"localization loss {losses_dict['Loss/localization_loss']:.8f}\")\n",
    "print(f\"classification loss {losses_dict['Loss/classification_loss']:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the model \n",
    "model.provide_groundtruth(groundtruth_boxes_list=[], groundtruth_classes_list=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorate with @tf.function for faster training \n",
    "@tf.function\n",
    "def train_step_fn(image_list,\n",
    "                groundtruth_boxes_list,\n",
    "                groundtruth_classes_list,\n",
    "                model,\n",
    "                optimizer,\n",
    "                vars_to_fine_tune, \n",
    "                step\n",
    "                ):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        preprocessed_image_list = []\n",
    "        true_shape_list = []\n",
    "\n",
    "        # Preprocess the images\n",
    "        for img in image_list:\n",
    "          processed_img, true_shape = model.preprocess(img)\n",
    "          preprocessed_image_list.append(processed_img)\n",
    "          true_shape_list.append(true_shape)\n",
    "   \n",
    "        preprocessed_image_tensor =  tf.concat(preprocessed_image_list, axis=0)\n",
    "        true_shape_tensor = tf.concat(true_shape_list, axis=0)\n",
    "\n",
    "        # Make a prediction\n",
    "        prediction_dict = model.predict(preprocessed_image_tensor, true_shape_tensor)\n",
    "\n",
    "        # Provide the ground truth to the model\n",
    "        model.provide_groundtruth(\n",
    "                    groundtruth_boxes_list=groundtruth_boxes_list,\n",
    "                    groundtruth_classes_list=groundtruth_classes_list)\n",
    "        \n",
    "        # Calculate the total loss (sum of both losses)\n",
    "        losses_dict = model.loss(prediction_dict, true_shape_tensor)\n",
    "            \n",
    "        total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
    "\n",
    "        # Calculate the gradients\n",
    "        gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
    "        \n",
    "        # Update learning rate.\n",
    "        # current_learning_rate = learning_rate_fn(step)\n",
    "        # optimizer.learning_rate.assign(current_learning_rate)\n",
    "\n",
    "        # Optimize the model's selected variables\n",
    "        optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
    "\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_batches=50 #300\n",
    "# optimizer.learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5cc3fe88464c49a545a2b6848d576a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/200 [00:00<?, ?Epoch/s, loss = {loss:.4f}, lr = {lr:.4f}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fine-tuning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740223603.617447  138570 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "# Presentation\n",
    "epochs = trange(\n",
    "    EPOCHS,\n",
    "    desc=\"Epoch\",\n",
    "    unit=\"Epoch\",\n",
    "    postfix=\"loss = {loss:.4f}, lr = {lr:.4f}\")\n",
    "epochs.set_postfix(loss=0, lr=0)\n",
    "\n",
    "\n",
    "print('Start fine-tuning!', flush=True)\n",
    "train_loss_results = []\n",
    "for _epoch in epochs:\n",
    "\n",
    "    # Grab keys for a random subset of examples\n",
    "    all_keys = list(range(len(train_images)))\n",
    "    random.shuffle(all_keys) \n",
    "    example_keys = all_keys[:batch_size]\n",
    "\n",
    "    # Get the ground truth\n",
    "    gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
    "    gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
    "\n",
    "    # get the images\n",
    "    image_tensors = [train_image_tensors[key] for key in example_keys]\n",
    "\n",
    "    # Training step (forward pass + backwards pass)\n",
    "    total_loss = train_step_fn(image_tensors,\n",
    "                               gt_boxes_list,\n",
    "                               gt_classes_list,\n",
    "                               model,\n",
    "                               optimizer,\n",
    "                               to_fine_tune,\n",
    "                               _epoch)\n",
    "\n",
    "    # if _epoch % 5 == 0:\n",
    "    # Presentation\n",
    "    _loss = total_loss.numpy()\n",
    "    lr = optimizer.learning_rate.numpy()\n",
    "    epochs.set_postfix(loss=float(_loss),\n",
    "                        lr=float(lr))\n",
    "        # print(f'{_epoch=} of {EPOCHS=}, loss={:5f}, lr={:5f}', flush=True)\n",
    "\n",
    "    train_loss_results.append(total_loss.numpy())\n",
    "print('Done fine-tuning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch 0 of 150, loss=1.3179475\n",
    "batch 10 of 150, loss=1.8767126\n",
    "batch 20 of 150, loss=0.7498219\n",
    "batch 30 of 150, loss=0.47309864\n",
    "batch 40 of 150, loss=0.283122\n",
    "batch 50 of 150, loss=0.21873556\n",
    "batch 60 of 150, loss=0.1488738\n",
    "batch 70 of 150, loss=0.13713343\n",
    "batch 80 of 150, loss=0.12227633\n",
    "batch 90 of 150, loss=0.10250672\n",
    "batch 100 of 150, loss=0.08533072\n",
    "batch 110 of 150, loss=0.07828214\n",
    "batch 120 of 150, loss=0.071007706\n",
    "batch 130 of 150, loss=0.07894869\n",
    "batch 140 of 150, loss=0.06765614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess, predict, and post process Test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load test images and run inference with new model!\n",
    "prepared_test_images = []\n",
    "# test_images, test_class_ids, test_bboxes \n",
    "for img in test_images:\n",
    "    prepared_test_images.append(np.expand_dims(img, axis=0))\n",
    "\n",
    "len(prepared_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, uncomment this decorator if you want to run inference eagerly\n",
    "@tf.function\n",
    "def detect(input_tensor):\n",
    "    \"\"\"Run detection on an input image.\n",
    "\n",
    "    Args:\n",
    "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
    "      Note that height and width can be anything since the image will be\n",
    "      immediately resized according to the needs of the model within this\n",
    "      function.\n",
    "\n",
    "    Returns:\n",
    "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
    "      and `detection_scores`).\n",
    "    \"\"\"\n",
    "    preprocessed_image, shapes = model.preprocess(input_tensor)\n",
    "    prediction_dict = model.predict(preprocessed_image, shapes)\n",
    "\n",
    "    # use the detection model's postprocess() method to get the the final detections\n",
    "    detections = model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_samples = random.sample(range(len(prepared_test_images)), 4)\n",
    "# random_samples\n",
    "len(prepared_test_images[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id_offset = 1\n",
    "results = {'boxes': [], 'scores': []}\n",
    "\n",
    "for i in range(len(prepared_test_images[:4])):\n",
    "    input_tensor = tf.convert_to_tensor(prepared_test_images[i], dtype=tf.float32)\n",
    "    detections = detect(input_tensor)\n",
    "    print(detections['detection_scores'][0].numpy())\n",
    "    plot_detections(\n",
    "      prepared_test_images[i][0],\n",
    "      detections['detection_boxes'][0].numpy(),\n",
    "      detections['detection_classes'][0].numpy().astype(np.uint32)\n",
    "      + label_id_offset,\n",
    "      detections['detection_scores'][0].numpy(),\n",
    "      category_index, \n",
    "      figsize=(15, 20))\n",
    "    results['boxes'].append(detections['detection_boxes'][0][0].numpy())\n",
    "    results['scores'].append(detections['detection_scores'][0][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections['detection_scores'][0].numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results['boxes']))\n",
    "print(results['boxes'][0].shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(prepared_test_images[:4])):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    input_tensor = tf.convert_to_tensor(prepared_test_images[i], dtype=tf.float32)\n",
    "    detections = detect(input_tensor)\n",
    "    plot_detections(\n",
    "      prepared_test_images[i][0],\n",
    "      detections['detection_boxes'][0].numpy(),\n",
    "      detections['detection_classes'][0].numpy().astype(np.uint32)\n",
    "      + label_id_offset,\n",
    "      detections['detection_scores'][0].numpy(),\n",
    "      category_index, figsize=(15, 20))\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inference results:\",  detections['detection_boxes'][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the figure size\n",
    "# plt.figure(figsize=(30, 15))\n",
    "\n",
    "# label_id_offset = 1\n",
    "# results = {'boxes': [], 'scores': []}\n",
    "\n",
    "# for idx, idx_img in enumerate(random_samples):\n",
    "#     plt.subplot(2, 4, idx+1)\n",
    "#     input_tensor = tf.convert_to_tensor(prepared_test_images[idx_img], dtype=tf.float32)\n",
    "#     detections = detect(input_tensor)\n",
    "#     print(detections['detection_boxes'][0].numpy()[idx_img])\n",
    "#     plot_detections(\n",
    "#       prepared_test_images[idx_img][0],\n",
    "#       detections['detection_boxes'][0].numpy()[idx_img],\n",
    "#       detections['detection_classes'][0].numpy().astype(np.uint32)[idx_img]\n",
    "#       + label_id_offset,\n",
    "#       detections['detection_scores'][0].numpy(),\n",
    "#       category_index, figsize=(15, 20))\n",
    "#     results['boxes'].append(detections['detection_boxes'][0][0].numpy())\n",
    "#     results['scores'].append(detections['detection_scores'][0][0].numpy())\n",
    "# plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the figure size\n",
    "plt.figure(figsize=(30, 15))\n",
    "\n",
    "# use the `plot_detections()` utility function to draw the ground truth boxes\n",
    "# random_samples = random.sample(range(len(train_images)), 5)\n",
    "# print(f\"Random samples: {random_samples}\")\n",
    "\n",
    "for i in range(len(prepared_test_images)):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plot_detections(\n",
    "      prepared_test_images[idx],\n",
    "      train_bboxes[idx],\n",
    "      np.ones(shape=[train_bboxes[idx].shape[0]], dtype=np.int32),\n",
    "      dummy_scores, category_index)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUTS_DIRS = Path(cfg.OUTPUTS.OUPUT_DIR)\n",
    "EXPORTER_SCRIPT = Path(cfg.OUTPUTS.EXPORTER_SCRIPT)\n",
    "\n",
    "CHECKPOINT_PATH = Path(cfg.OUTPUTS.CHECKPOINT_PATH)\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('main_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.Model.save(model, f'my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_obj   = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "ckpt_obj.save(file_prefix=f'{CHECKPOINT_PATH}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec(shape=[None, 640, 640, 3], dtype=tf.float32)])\n",
    "def detect_fn(input_tensor):\n",
    "    preprocessed_image, shapes = model.preprocess(input_tensor)\n",
    "    prediction_dict = model.predict(preprocessed_image, shapes)\n",
    "\n",
    "    # use the detection model's postprocess() method to get the the final detections\n",
    "    detections = model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "# Save with signatures\n",
    "tf.saved_model.save(model, f'{OUTPUTS_DIRS}', signatures={\"serving_default\": detect_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the exported model\n",
    "loaded_model = tf.saved_model.load(f'{OUTPUTS_DIRS}')\n",
    "\n",
    "# Perform inference\n",
    "input_tensor = tf.random.uniform([1, 640, 640, 3])  # Example input tensor\n",
    "detections = loaded_model.signatures[\"serving_default\"](input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUTS_DIRS    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command = f\"python3 {EXPORTER_SCRIPT} \\\n",
    "#     --input_type=image_tensor \\\n",
    "#     --pipeline_config_path={CONFIG_PIPELINE_PATH/'pipeline.config'} \\\n",
    "#     --trained_checkpoint_dir={CHECKPOINT_PATH} \\\n",
    "#     --output_directory={OUTPUTS_DIRS}\"\n",
    "# command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPELINE_PATH/'pipeline.config'))\n",
    "# configs['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _model = tf.keras.Model()\n",
    "# optimizer = tf.keras.optimizers.SGD()\n",
    "# model_path = str(OUTPUTS_DIRS)\n",
    "# _ckpt_obj   = tf.train.Checkpoint(optimizer=optimizer, model=_model)\n",
    "# _ckpt_obj.restore(save_path=tf.train.latest_checkpoint(str(CHECKPOINT_PATH))).assert_consumed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
